# AI Prompts Configuration

classification_prompt: |
  You are an expert QA engineer analyzing test automation failures from a Selenium + Java + TestNG framework.
  
  Your task: Classify each test failure as PRODUCT_BUG, AUTOMATION_ISSUE, or PRODUCT_CHANGE.
  
  PRODUCT_BUG indicators:
  - Assertion failures on business logic (expected vs actual values don't match)
  - Unexpected application behavior or state
  - API returning incorrect data or status codes
  - UI displaying wrong values or content
  - Functional defects in the application
  - Database data inconsistencies
  - Business rule violations
  
  AUTOMATION_ISSUE indicators:
  - NoSuchElementException (element locator not found)
  - TimeoutException (element didn't appear in time)
  - StaleElementReferenceException (DOM changed)
  - WebDriver connection/session issues
  - Test data setup/cleanup failures
  - Flaky tests (intermittent failures)
  - Environment/infrastructure issues (server down, network issues)
  - Synchronization problems (race conditions)
  - Test framework configuration issues
  
  PRODUCT_CHANGE indicators:
  - Test failure due to intentional product changes (expected behavior change)
  - API response structure changed (new fields, renamed fields, removed fields)
  - UI elements changed (new design, renamed IDs/classes, removed elements)
  - Business logic updated (workflow changes, new validations, updated rules)
  - Test expectations need to be updated to match new product behavior
  - The failure indicates the product works correctly but test needs updating
  
  For each failure, provide a JSON response with:
  {{
    "classification": "PRODUCT_BUG" or "AUTOMATION_ISSUE" or "PRODUCT_CHANGE",
    "confidence": "HIGH" or "MEDIUM" or "LOW",
    "root_cause": "EXTRACT EXACT DETAILS. Do not summarize. Must include: API Name, Status Code, Missing Key, Locator Name, or Exception Message. Example: 'POST /api/v1/user returned 500' or 'Missing key: phoneNumber' or 'Locator: #submit-btn not found'",
    "recommended_action": "Specific next step to take"
  }}
  
  Test Failure Details:
  {failure_details}
  
  Respond ONLY with valid JSON, no additional text.

summary_prompt: |
  Generate a concise executive summary of today's test automation results.
  
  Test Execution Summary:
  - Total Tests: {total_tests}
  - Passed: {passed}
  - Failed: {failed}
  - Skipped: {skipped}
  - Pass Rate: {pass_rate}%
  
  Failure Analysis:
  {failures_analysis}
  
  Historical Context:
  {historical_context}
  
  Create a professional summary with these sections:
  
  1. **Executive Summary** (2-3 sentences)
     - Overall test health
     - Key highlights or concerns
  
  2. **Critical Product Bugs** (if any)
     - List bugs requiring immediate attention
     - Include test name and brief description
     - Prioritize by severity
  
  3. **Automation Issues to Fix** (if any)
     - List automation framework problems
     - Include test name and fix recommendation
     - Prioritize by impact (blocking multiple tests, flaky, etc.)
  
  4. **Trends & Insights**
     - Compare to previous runs
     - Identify recurring failures
     - Note improvements or regressions
  
  5. **Recommended Actions**
     - Top 3 priorities for the team
     - Specific, actionable items
  
  Use clear, professional language. Be concise but informative.

recurring_analysis_prompt: |
  Analyze these recurring test failures that have failed {failure_count} times in the last {days} days:
  
  {recurring_failures}
  
  For each recurring failure, determine:
  1. Is this a persistent product bug or a flaky automation issue?
  2. What is the likely root cause?
  3. Why hasn't it been fixed yet? (test environment issue, low priority, etc.)
  4. What's the recommended resolution strategy?
  
  Provide actionable insights to help the team prioritize fixes.
